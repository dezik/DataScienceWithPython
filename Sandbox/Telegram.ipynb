{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdatasciencecondae8f42ab0efea4f998c891fa6287f9d0d",
   "display_name": "Python 3.7.6 64-bit ('datascience': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read chat to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../Data/dva_dimona_vovonjaka.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allChats = json_normalize(data.chats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = json_normalize(allChats.messages[0])\n",
    "chat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = chat[chat['type']=='message']\n",
    "chat = chat[['id', 'date', 'edited', 'text', 'from', 'media_type', 'sticker_emoji', 'reply_to_message_id']]\n",
    "chat['date'] = chat['date'].apply(lambda x: pd.Timestamp(x))\n",
    "chat['edited'] = chat['edited'].apply(lambda x: pd.Timestamp(x))\n",
    "chat.replace(pd.Timestamp('1970-01-01T01:00:00'), pd.NA, inplace=True)\n",
    "chat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat[['text', 'from', 'date']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics about message types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=4, figsize = (12,4))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "chat['from'].value_counts().plot(kind='bar', title='Количество сообщений', ax=axes[0])\n",
    "chat[chat.media_type == 'sticker']['from'].value_counts().plot(kind='bar', title='Количество стикеров', ax=axes[1])\n",
    "chat[chat.text.apply(lambda x: not isinstance(x, str))]['from'].value_counts().plot(kind='bar', title='Количество ссылок', ax=axes[2])\n",
    "chat[chat.media_type == 'video_file']['from'].value_counts().plot(kind='bar', title='Количество видео', ax=axes[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extract data from non-text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = np.array([])\n",
    "mentions = np.array([])\n",
    "links = np.array([])\n",
    "for l in chat[chat.text.apply(lambda x: isinstance(x, list))].text.values:\n",
    "    for item in l:\n",
    "        if isinstance(item, str):\n",
    "            msgs = np.append(item, msgs)\n",
    "        if isinstance(item, dict):\n",
    "            if item['type'] == 'link':\n",
    "                links = np.append(item['text'], links)\n",
    "            if item['type'] == 'mention':\n",
    "                mentions = np.append(item['text'], mentions)\n",
    "                # mentions.append(item['text'])\n",
    "\n",
    "print('Messages: ', msgs)\n",
    "print()\n",
    "print('Mentions: ', mentions)\n",
    "print()\n",
    "print('Links: ', links)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define tokenixzation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "mystem = Mystem() \n",
    "def tokenize(text:str, stopWords: []):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "\n",
    "    for p in punctuation:\n",
    "        if p != '-':\n",
    "            tokens = [t.replace(p, \" \") for t in tokens]\n",
    "\n",
    "    return [token.strip() for token in tokens if token.strip() not in stopWords\\\n",
    "              and token.strip() != \"\" \\\n",
    "              and not token.isdigit()\n",
    "              and token.strip() >= 'А'\n",
    "              and token.strip() <='я'\n",
    "              and token.strip() not in punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Define stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', '–', 'к', 'на', '...', '✔', '•', '’', 'че', 'ток', 'шо', 'тип'])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Difine function for getting user vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def getTokens(data, stopWords=[]):\n",
    "    # tokenize raw text messages\n",
    "    messages = data[data.apply(lambda x: isinstance(x, str))]\n",
    "    tokens = tokenize(' '.join(messages), stopWords)\n",
    "\n",
    "    # tokenize text from complex messages\n",
    "    nonMessages = data[data.apply(lambda x: isinstance(x, list))]\n",
    "    msgs = np.array([])\n",
    "    for l in nonMessages.values:\n",
    "        for item in l:\n",
    "            if isinstance(item, str):\n",
    "                msgs = np.append(item, msgs)\n",
    "    tokens.extend(tokenize(' '.join(msgs), stopWords))\n",
    "    return tokens\n",
    "\n",
    "def getUserVocabulary(sender:str, stopWords=[]):\n",
    "    allMsgs = chat[chat['from'] == sender].text\n",
    "    tokens = getTokens(allMsgs, stopWords)\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get user most used words statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize = (15,4))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "z_words, z_counts = zip(*getUserVocabulary('Dez Dezsson').most_common(20))\n",
    "d_words, d_counts = zip(*getUserVocabulary('Дима Лацюга').most_common(20))\n",
    "v_words, w_counts = zip(*getUserVocabulary('Вова Свинухов').most_common(20))\n",
    "\n",
    "axes[0].set_title('Dez Dezsson')\n",
    "axes[0].barh(z_words, z_counts)\n",
    "\n",
    "axes[1].set_title('Дима Лацюга')\n",
    "axes[1].barh(d_words, d_counts)\n",
    "\n",
    "axes[2].set_title('Вова Свинухов')\n",
    "axes[2].barh(v_words, w_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize = (15,4))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "z_words, z_counts = zip(*getUserVocabulary('Dez Dezsson', stop_words).most_common(20))\n",
    "d_words, d_counts = zip(*getUserVocabulary('Дима Лацюга', stop_words).most_common(20))\n",
    "v_words, w_counts = zip(*getUserVocabulary('Вова Свинухов', stop_words).most_common(20))\n",
    "\n",
    "axes[0].set_title('Dez Dezsson')\n",
    "axes[0].barh(z_words, z_counts)\n",
    "\n",
    "axes[1].set_title('Дима Лацюга')\n",
    "axes[1].barh(d_words, d_counts)\n",
    "\n",
    "axes[2].set_title('Вова Свинухов')\n",
    "axes[2].barh(v_words, w_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_len=len(getUserVocabulary('Dez Dezsson', stop_words).keys())\n",
    "d_len=len(getUserVocabulary('Дима Лацюга', stop_words).keys())\n",
    "v_len=len(getUserVocabulary('Вова Свинухов', stop_words).keys())\n",
    "plt.bar(['Dez Dezsson', 'Дима Лацюга', 'Вова Свинухов'], [z_len, d_len, v_len])\n",
    "plt.title(\"Словарный запас\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_count = len(chat[chat['from'] == 'Dez Dezsson'])\n",
    "d_count = len(chat[chat['from'] == 'Дима Лацюга'])\n",
    "v_count = len(chat[chat['from'] == 'Вова Свинухов'])\n",
    "\n",
    "print(z_count, d_count, v_count)\n",
    "\n",
    "plt.bar(['Dez Dezsson', 'Дима Лацюга', 'Вова Свинухов'], [z_len/z_count, d_len/d_count, v_len/v_count])\n",
    "plt.title(\"Словарный запас vs колличество сообщений\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find n-gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNGramms(data, n=1,most_common=20,stop_words=[]):\n",
    "    tokens = getTokens(data, stop_words)\n",
    "    words=nltk.ngrams(tokens,n)\n",
    "    words=nltk.FreqDist(words)\n",
    "    print ('Колличество токенов: ',words.N())\n",
    "    print ('Колличество уникальных токенов: ',words.B())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N-граммы Димона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Дима Лацюга'].text, n=2, stop_words=stop_words).plot(20, title = 'Дима Лацюга 2-gramms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Дима Лацюга'].text, n=3, stop_words=stop_words).plot(20, title = 'Дима Лацюга 3-gramms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Дима Лацюга'].text, n=4, stop_words=stop_words).plot(20, title = 'Дима Лацюга 4-gramms')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N-граммы Вовоняки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Вова Свинухов'].text, n=2, stop_words=stop_words).plot(20, title = 'Вова Свинухов 2-gramms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Вова Свинухов'].text, n=3, stop_words=stop_words).plot(20, title = 'Вова Свинухов 3-gramms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Вова Свинухов'].text, n=4, stop_words=stop_words).plot(20, title = 'Вова Свинухов 4-gramms')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Мои N-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat[chat['from'] == 'Dez Dezsson'].text, n=2, stop_words=stop_words).plot(20, title = 'Dez Dezsson 2-gramms')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNGramms(chat.text, n=2, stop_words=stop_words).plot(20, title = 'Наиболее частые 2-gramms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "s = 'Умирает старый пчеловод, посвятивший всю жизнь разведению пчёл и уходу за ними. Лёжа на смертном одре, он подзывает к себе трёх своих сыновей и говорит: «Дети мои! Запомните – всё в этой жизни хуйня – всё кроме пчёл» Затем, закрыв глаза и помолчав немного, добавляет: «А впрочем, и пчёлы тоже хуйня».'\n",
    "\n",
    "\n",
    "sentences = [tokenize(sent) for sent in sent_tokenize(s, 'russian')]\n",
    "print(sentences) \n",
    "# model = gensim.models.Word2Vec(sentences, size=150, window=5, min_count=5, workers=4)\n",
    "# model.save('./w2v.model')\n",
    "# print('saved')\n",
    "\n",
    "# tokens = tokenize(s)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}